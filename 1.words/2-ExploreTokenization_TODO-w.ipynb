{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook outlines several methods for tokenizing text into words (and sentences), including:\n",
    "\n",
    "* whitespace\n",
    "* nltk (Penn Treebank tokenizer)\n",
    "* nltk (Twitter-aware)\n",
    "* spaCy\n",
    "* custom regular expressions\n",
    "\n",
    "highlighting differences between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, json\n",
    "import spacy\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/pierrejaumier/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you haven't downloaded the NLTK sentence segmentation model before, do so here\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy lemmatization needs tagger but disable the rest\n",
    "nlp = spacy.load('en', disable=['tagger,ner,parser'])\n",
    "nlp.remove_pipe('tagger')\n",
    "nlp.remove_pipe('ner')\n",
    "nlp.remove_pipe('parser');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tweets_from_json(filename):\n",
    "    tweets=[]\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        data=json.load(file)\n",
    "        for tweet in data:\n",
    "            tweets.append(tweet[\"text\"])\n",
    "    return tweets        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trump_tweets.json comes from the Trump Twitter collection here (downloaded 1/19/19)\n",
    "http://www.trumptwitterarchive.com/archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"../data/trump_tweets.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=read_tweets_from_json(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitespace_tokens=[]\n",
    "for tweet in tweets:\n",
    "    whitespace_tokens.append(tweet.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_tokens=[]\n",
    "for tweet in tweets:\n",
    "    nltk_tokens.append(nltk.word_tokenize(tweet, language=\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_casual_tokens=[]\n",
    "for tweet in tweets:\n",
    "    nltk_casual_tokens.append(nltk.casual_tokenize(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokens=[]\n",
    "for tweet in tweets:\n",
    "    spacy_tokens.append([token.text for token in nlp(tweet)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shorter version of http://sentiment.christopherpotts.net/code-data/happyfuntokenizing.py\n",
    "\n",
    "# The order here is important (match from first to last)\n",
    "\n",
    "# Keep usernames together (any token starting with @, followed by A-Z, a-z, 0-9)\n",
    "regexes=(r\"(?:@[\\w_]+)\",\n",
    "\n",
    "# Keep hashtags together (any token starting with #, followed by A-Z, a-z, 0-9, _, or -)\n",
    "r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\",\n",
    "\n",
    "# Keep words with apostrophes, hyphens and underscores together\n",
    "r\"(?:[a-z][a-z’'\\-_]+[a-z])\",\n",
    "\n",
    "# Keep all other sequences of A-Z, a-z, 0-9, _ together\n",
    "r\"(?:[\\w_]+)\",\n",
    "\n",
    "# Everything else that's not whitespace\n",
    "r\"(?:\\S)\"\n",
    ")\n",
    "\n",
    "big_regex=\"|\".join(regexes)\n",
    "\n",
    "my_extensible_tokenizer = re.compile(big_regex, re.VERBOSE | re.I | re.UNICODE)\n",
    "\n",
    "def my_extensible_tokenize(text):\n",
    "    return my_extensible_tokenizer.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "extensible_tokens=[]\n",
    "for tweet in tweets:\n",
    "    extensible_tokens.append(my_extensible_tokenize(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Write a function to print out the first 5 tokenized tweets in each of the five tokenizers above. Examine those tweets; how would you characterize the differences?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mexico is doing NOTHING to stop the Caravan which is now fully formed and heading to the United States. We stopped the last two - many are still in Mexico but can’t get through our Wall, but it takes a lot of Border Agents if there is no Wall. Not easy!'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mexico',\n",
       " 'is',\n",
       " 'doing',\n",
       " 'NOTHING',\n",
       " 'to',\n",
       " 'stop',\n",
       " 'the',\n",
       " 'Caravan',\n",
       " 'which',\n",
       " 'is',\n",
       " 'now',\n",
       " 'fully',\n",
       " 'formed',\n",
       " 'and',\n",
       " 'heading',\n",
       " 'to',\n",
       " 'the',\n",
       " 'United',\n",
       " 'States.',\n",
       " 'We',\n",
       " 'stopped',\n",
       " 'the',\n",
       " 'last',\n",
       " 'two',\n",
       " '-',\n",
       " 'many',\n",
       " 'are',\n",
       " 'still',\n",
       " 'in',\n",
       " 'Mexico',\n",
       " 'but',\n",
       " 'can’t',\n",
       " 'get',\n",
       " 'through',\n",
       " 'our',\n",
       " 'Wall,',\n",
       " 'but',\n",
       " 'it',\n",
       " 'takes',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'Border',\n",
       " 'Agents',\n",
       " 'if',\n",
       " 'there',\n",
       " 'is',\n",
       " 'no',\n",
       " 'Wall.',\n",
       " 'Not',\n",
       " 'easy!']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whitespace_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ('a', 'b')\n",
      "1 ('aa', 'bb')\n"
     ]
    }
   ],
   "source": [
    "a = ['a', 'aa']\n",
    "b = ['b', 'bb', 'bbb']\n",
    "for i,x in enumerate(zip(a,b)):\n",
    "    print(i,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a aa'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formatage en python: https://realpython.com/python-f-strings/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, Eric. You are 74.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = \"Eric\"\n",
    "age = 74\n",
    "f\"Hello, {name}. You are {age}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whitespace >\n",
      " Mexico is doing NOTHING to stop the Caravan which is now fully formed and heading to the United States. We stopped the last two - many are still in Mexico but can’t get through our Wall, but it takes a lot of Border Agents if there is no Wall. Not easy!\n",
      "nltk >\n",
      " Mexico is doing NOTHING to stop the Caravan which is now fully formed and heading to the United States . We stopped the last two - many are still in Mexico but can ’ t get through our Wall , but it takes a lot of Border Agents if there is no Wall . Not easy !\n",
      "nltk_casual >\n",
      " Mexico is doing NOTHING to stop the Caravan which is now fully formed and heading to the United States . We stopped the last two - many are still in Mexico but can ’ t get through our Wall , but it takes a lot of Border Agents if there is no Wall . Not easy !\n",
      "spacy >\n",
      " Mexico is doing NOTHING to stop the Caravan which is now fully formed and heading to the United States . We stopped the last two - many are still in Mexico but ca n’t get through our Wall , but it takes a lot of Border Agents if there is no Wall . Not easy !\n",
      "extensible >\n",
      " Mexico is doing NOTHING to stop the Caravan which is now fully formed and heading to the United States . We stopped the last two - many are still in Mexico but can’t get through our Wall , but it takes a lot of Border Agents if there is no Wall . Not easy !\n",
      "\n",
      "\n",
      "whitespace >\n",
      " Many people are saying that the Mainstream Media will have a very hard time restoring credibility because of the way they have treated me over the past 3 years (including the election lead-up), as highlighted by the disgraceful Buzzfeed story &amp; the even more disgraceful coverage!\n",
      "nltk >\n",
      " Many people are saying that the Mainstream Media will have a very hard time restoring credibility because of the way they have treated me over the past 3 years ( including the election lead-up ) , as highlighted by the disgraceful Buzzfeed story & amp ; the even more disgraceful coverage !\n",
      "nltk_casual >\n",
      " Many people are saying that the Mainstream Media will have a very hard time restoring credibility because of the way they have treated me over the past 3 years ( including the election lead-up ) , as highlighted by the disgraceful Buzzfeed story & the even more disgraceful coverage !\n",
      "spacy >\n",
      " Many people are saying that the Mainstream Media will have a very hard time restoring credibility because of the way they have treated me over the past 3 years ( including the election lead - up ) , as highlighted by the disgraceful Buzzfeed story & amp ; the even more disgraceful coverage !\n",
      "extensible >\n",
      " Many people are saying that the Mainstream Media will have a very hard time restoring credibility because of the way they have treated me over the past 3 years ( including the election lead-up ) , as highlighted by the disgraceful Buzzfeed story & amp ; the even more disgraceful coverage !\n",
      "\n",
      "\n",
      "whitespace >\n",
      " The Economy is one of the best in our history, with unemployment at a 50 year low, and the Stock Market ready to again break a record (set by us many times) - &amp; all you heard yesterday, based on a phony story, was Impeachment. You want to see a Stock Market Crash, Impeach Trump!\n",
      "nltk >\n",
      " The Economy is one of the best in our history , with unemployment at a 50 year low , and the Stock Market ready to again break a record ( set by us many times ) - & amp ; all you heard yesterday , based on a phony story , was Impeachment . You want to see a Stock Market Crash , Impeach Trump !\n",
      "nltk_casual >\n",
      " The Economy is one of the best in our history , with unemployment at a 50 year low , and the Stock Market ready to again break a record ( set by us many times ) - & all you heard yesterday , based on a phony story , was Impeachment . You want to see a Stock Market Crash , Impeach Trump !\n",
      "spacy >\n",
      " The Economy is one of the best in our history , with unemployment at a 50 year low , and the Stock Market ready to again break a record ( set by us many times ) - & amp ; all you heard yesterday , based on a phony story , was Impeachment . You want to see a Stock Market Crash , Impeach Trump !\n",
      "extensible >\n",
      " The Economy is one of the best in our history , with unemployment at a 50 year low , and the Stock Market ready to again break a record ( set by us many times ) - & amp ; all you heard yesterday , based on a phony story , was Impeachment . You want to see a Stock Market Crash , Impeach Trump !\n",
      "\n",
      "\n",
      "whitespace >\n",
      " .@newtgingrich just stated that there has been no president since Abraham Lincoln who has been treated worse or more unfairly by the media than your favorite President, me! At the same time there has been no president who has accomplished more in his first two years in office!\n",
      "nltk >\n",
      " . @ newtgingrich just stated that there has been no president since Abraham Lincoln who has been treated worse or more unfairly by the media than your favorite President , me ! At the same time there has been no president who has accomplished more in his first two years in office !\n",
      "nltk_casual >\n",
      " . @newtgingrich just stated that there has been no president since Abraham Lincoln who has been treated worse or more unfairly by the media than your favorite President , me ! At the same time there has been no president who has accomplished more in his first two years in office !\n",
      "spacy >\n",
      " .@newtgingrich just stated that there has been no president since Abraham Lincoln who has been treated worse or more unfairly by the media than your favorite President , me ! At the same time there has been no president who has accomplished more in his first two years in office !\n",
      "extensible >\n",
      " . @newtgingrich just stated that there has been no president since Abraham Lincoln who has been treated worse or more unfairly by the media than your favorite President , me ! At the same time there has been no president who has accomplished more in his first two years in office !\n",
      "\n",
      "\n",
      "whitespace >\n",
      " Will be leaving for Dover to be with the families of 4 very special people who lost their lives in service to our Country!\n",
      "nltk >\n",
      " Will be leaving for Dover to be with the families of 4 very special people who lost their lives in service to our Country !\n",
      "nltk_casual >\n",
      " Will be leaving for Dover to be with the families of 4 very special people who lost their lives in service to our Country !\n",
      "spacy >\n",
      " Will be leaving for Dover to be with the families of 4 very special people who lost their lives in service to our Country !\n",
      "extensible >\n",
      " Will be leaving for Dover to be with the families of 4 very special people who lost their lives in service to our Country !\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, (whitespace, nltk, nltk_casual, spacy, extensible) \\\n",
    "    in enumerate(zip(whitespace_tokens, nltk_tokens, nltk_casual_tokens, spacy_tokens, extensible_tokens)):\n",
    "    if idx >= 5:\n",
    "        break\n",
    "    print(f\"whitespace >\\n {' '.join(whitespace_tokens[idx])}\")\n",
    "    print(f\"nltk >\\n {' '.join(nltk_tokens[idx])}\")\n",
    "    print(f\"nltk_casual >\\n {' '.join(nltk_casual_tokens[idx])}\")\n",
    "    print(f\"spacy >\\n {' '.join(spacy_tokens[idx])}\")\n",
    "    print(f\"extensible >\\n {' '.join(extensible_tokens[idx])}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: Write a function `compare(tokenization_one, tokenization_two)` that compares two tokenizations of the same text and finds the 20 most frequent tokens that don't appear in the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('@', 39951),\n",
       " ('.', 32387),\n",
       " (':', 23593),\n",
       " ('!', 21713),\n",
       " ('the', 21222),\n",
       " (',', 20257),\n",
       " ('to', 14812),\n",
       " ('``', 13299),\n",
       " (\"''\", 11514),\n",
       " ('and', 11218),\n",
       " ('a', 10622),\n",
       " ('is', 9904),\n",
       " ('of', 9768),\n",
       " ('in', 8773),\n",
       " ('realDonaldTrump', 8655),\n",
       " ('I', 8642),\n",
       " ('you', 7774),\n",
       " ('for', 7639),\n",
       " ('#', 7333),\n",
       " ('on', 6268)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fréquence d'éléments dans une liste de listes\n",
    "l = nltk_tokens\n",
    "flat_list = [item for sublist in l for item in sublist]\n",
    "Counter(flat_list).most_common(20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(tokenization_one, tokenization_two):\n",
    "    flat_list_one = [token for sublist in tokenization_one for token in sublist]\n",
    "    flat_list_two = [token for sublist in tokenization_two for token in sublist]\n",
    "    # Eléments uniques dans chacune des listes\n",
    "    set_one = set(flat_list_one)\n",
    "    set_two = set(flat_list_two)\n",
    "    \n",
    "    # Liste d'éléments non vus dans l'autre tokenization\n",
    "    unseen_token = []\n",
    "    for token in flat_list_one:\n",
    "        if token not in set_two:\n",
    "            unseen_token.append(token)\n",
    "    for token in flat_list_two:\n",
    "        if token not in set_one:\n",
    "            unseen_token.append(token)\n",
    "    return Counter(unseen_token).most_common(20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\t24807\n",
      "``\t13299\n",
      "''\t11514\n",
      "@realDonaldTrump\t8661\n",
      "'s\t3541\n",
      "amp\t3364\n",
      "n't\t2503\n",
      "--\t2077\n",
      "Trump2016\t846\n",
      "#Trump2016\t840\n",
      "@BarackObama\t732\n",
      "U.S.\t665\n",
      "don't\t626\n",
      "#MakeAmericaGreatAgain\t560\n",
      "@FoxNews\t547\n",
      "....\t542\n",
      "'m\t538\n",
      "'re\t528\n",
      "I'm\t524\n",
      "@foxandfriends\t504\n"
     ]
    }
   ],
   "source": [
    "for key, value in compare(nltk_casual_tokens, nltk_tokens):\n",
    "    print(f\"{key}\\t{value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: Use one of the NLTK tokenizers; write code to determine how many sentences are in this dataset, and what the average number of words per sentence is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aide: https://www.guru99.com/tokenize-words-sentences-nltk.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mexico is doing NOTHING to stop the Caravan which is now fully formed and heading to the United States. We stopped the last two - many are still in Mexico but can’t get through our Wall, but it takes a lot of Border Agents if there is no Wall. Not easy!'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['God is Great!', 'I won a lottery.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"God is Great! I won a lottery.\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text = tweets[0]\n",
    "print(len(sent_tokenize(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de phrases 70491\n",
      "Moyenne de mots par phrase 12.55\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "n_sentences = 0\n",
    "word_per_sentence = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    sentences = sent_tokenize(tweet)\n",
    "    n_sentences += len(sentences)\n",
    "    for sentence in sentences:\n",
    "        word_per_sentence.append(len(word_tokenize(sentence)))\n",
    "\n",
    "print(f\"Nombre total de phrases {n_sentences}\")\n",
    "print(f\"Moyenne de mots par phrase {sum(word_per_sentence)/len(word_per_sentence):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4 (check-plus): modify the extensible tokenizer above to keep urls together (e.g., www.google.com or http://www.google.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep usernames together (any token starting with @, followed by A-Z, a-z, 0-9)\n",
    "regexes=(r\"(?:@[\\w_]+)\",\n",
    "\n",
    "# Keep hashtags together (any token starting with #, followed by A-Z, a-z, 0-9, _, or -)\n",
    "r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\",\n",
    "\n",
    "# Keep urls together\n",
    "r\"(?:https?:\\S+)\",\n",
    "r\"(?:www\\.\\S+)\",\n",
    "         \n",
    "# Keep words with apostrophes, hyphens and underscores together\n",
    "r\"(?:[a-z][a-z’'\\-_]+[a-z])\",\n",
    "\n",
    "# Keep all other sequences of A-Z, a-z, 0-9, _ together\n",
    "r\"(?:[\\w_]+)\",\n",
    "\n",
    "# Everything else that's not whitespace\n",
    "r\"(?:\\S)\"\n",
    ")\n",
    "\n",
    "big_regex=\"|\".join(regexes)\n",
    "\n",
    "my_url_extensible_tokenizer = re.compile(big_regex, re.VERBOSE | re.I | re.UNICODE)\n",
    "\n",
    "def my_extensible_tokenize_with_urls(text):\n",
    "    return my_url_extensible_tokenizer.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'course',\n",
       " 'website',\n",
       " 'is',\n",
       " 'http://people.ischool.berkeley.edu/~dbamman/info256.html']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_extensible_tokenize_with_urls(\"The course website is http://people.ischool.berkeley.edu/~dbamman/info256.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "course\n",
      "website\n",
      "is\n",
      "http://people.ischool.berkeley.edu/~dbamman/info256.html\n"
     ]
    }
   ],
   "source": [
    "print ('\\n'.join(my_extensible_tokenize_with_urls(\"The course website is http://people.ischool.berkeley.edu/~dbamman/info256.html\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
