{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook outlines several methods for tokenizing text into words (and sentences), including:\n",
    "\n",
    "* whitespace\n",
    "* nltk (Penn Treebank tokenizer)\n",
    "* nltk (Twitter-aware)\n",
    "* spaCy\n",
    "* custom regular expressions\n",
    "\n",
    "highlighting differences between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, json\n",
    "import spacy\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/pierrejaumier/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you haven't downloaded the sentence segmentation model before, do so here\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy lemmatization needs tagger but disable the rest\n",
    "nlp = spacy.load('en', disable=['tagger,ner,parser'])\n",
    "nlp.remove_pipe('tagger')\n",
    "nlp.remove_pipe('ner')\n",
    "nlp.remove_pipe('parser');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tweets_from_json(filename):\n",
    "    tweets=[]\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        data=json.load(file)\n",
    "        for tweet in data:\n",
    "            tweets.append(tweet[\"text\"])\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trump_tweets.json comes from the Trump Twitter collection here (downloaded 1/19/19) http://www.trumptwitterarchive.com/archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"../data/trump_tweets.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=read_tweets_from_json(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitespace_tokens=[]\n",
    "for tweet in tweets:\n",
    "    whitespace_tokens.append(tweet.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_tokens=[]\n",
    "for tweet in tweets:\n",
    "    nltk_tokens.append(nltk.word_tokenize(tweet, language=\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_casual_tokens=[]\n",
    "for tweet in tweets:\n",
    "    nltk_casual_tokens.append(nltk.casual_tokenize(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokens=[]\n",
    "for tweet in tweets:\n",
    "    spacy_tokens.append([token.text for token in nlp(tweet)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shorter version of http://sentiment.christopherpotts.net/code-data/happyfuntokenizing.py\n",
    "\n",
    "# The order here is important (match from first to last)\n",
    "\n",
    "# Keep usernames together (any token starting with @, followed by A-Z, a-z, 0-9)\n",
    "regexes=(r\"(?:@[\\w_]+)\",\n",
    "\n",
    "# Keep hashtags together (any token starting with #, followed by A-Z, a-z, 0-9, _, or -)\n",
    "r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\",\n",
    "\n",
    "# Keep words with apostrophes, hyphens and underscores together\n",
    "r\"(?:[a-z][a-z’'\\-_]+[a-z])\",\n",
    "\n",
    "# Keep all other sequences of A-Z, a-z, 0-9, _ together\n",
    "r\"(?:[\\w_]+)\",\n",
    "\n",
    "# Everything else that's not whitespace\n",
    "r\"(?:\\S)\"\n",
    ")\n",
    "\n",
    "big_regex=\"|\".join(regexes)\n",
    "\n",
    "my_extensible_tokenizer = re.compile(big_regex, re.VERBOSE | re.I | re.UNICODE)\n",
    "\n",
    "def my_extensible_tokenize(text):\n",
    "    return my_extensible_tokenizer.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "extensible_tokens=[]\n",
    "for tweet in tweets:\n",
    "    extensible_tokens.append(my_extensible_tokenize(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Write a function to print out the first 5 tokenized tweets in each of the five tokenizers above. Examine those tweets; how would you characterize the differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK      :\tMexico is doing NOTHING to stop the Caravan which is now fully formed and heading to the United States . We stopped the last two - many are still in Mexico but can ’ t get through our Wall , but it takes a lot of Border Agents if there is no Wall . Not easy !\n",
      "CASUAL    :\tMexico is doing NOTHING to stop the Caravan which is now fully formed and heading to the United States . We stopped the last two - many are still in Mexico but can ’ t get through our Wall , but it takes a lot of Border Agents if there is no Wall . Not easy !\n",
      "SPACY     :\tMexico is doing NOTHING to stop the Caravan which is now fully formed and heading to the United States . We stopped the last two - many are still in Mexico but ca n’t get through our Wall , but it takes a lot of Border Agents if there is no Wall . Not easy !\n",
      "WHITESPACE:\tMexico is doing NOTHING to stop the Caravan which is now fully formed and heading to the United States. We stopped the last two - many are still in Mexico but can’t get through our Wall, but it takes a lot of Border Agents if there is no Wall. Not easy!\n",
      "EXTENSIBLE:\tMexico is doing NOTHING to stop the Caravan which is now fully formed and heading to the United States . We stopped the last two - many are still in Mexico but can’t get through our Wall , but it takes a lot of Border Agents if there is no Wall . Not easy !\n",
      "\n",
      "NLTK      :\tMany people are saying that the Mainstream Media will have a very hard time restoring credibility because of the way they have treated me over the past 3 years ( including the election lead-up ) , as highlighted by the disgraceful Buzzfeed story & amp ; the even more disgraceful coverage !\n",
      "CASUAL    :\tMany people are saying that the Mainstream Media will have a very hard time restoring credibility because of the way they have treated me over the past 3 years ( including the election lead-up ) , as highlighted by the disgraceful Buzzfeed story & the even more disgraceful coverage !\n",
      "SPACY     :\tMany people are saying that the Mainstream Media will have a very hard time restoring credibility because of the way they have treated me over the past 3 years ( including the election lead - up ) , as highlighted by the disgraceful Buzzfeed story & amp ; the even more disgraceful coverage !\n",
      "WHITESPACE:\tMany people are saying that the Mainstream Media will have a very hard time restoring credibility because of the way they have treated me over the past 3 years (including the election lead-up), as highlighted by the disgraceful Buzzfeed story &amp; the even more disgraceful coverage!\n",
      "EXTENSIBLE:\tMany people are saying that the Mainstream Media will have a very hard time restoring credibility because of the way they have treated me over the past 3 years ( including the election lead-up ) , as highlighted by the disgraceful Buzzfeed story & amp ; the even more disgraceful coverage !\n",
      "\n",
      "NLTK      :\tThe Economy is one of the best in our history , with unemployment at a 50 year low , and the Stock Market ready to again break a record ( set by us many times ) - & amp ; all you heard yesterday , based on a phony story , was Impeachment . You want to see a Stock Market Crash , Impeach Trump !\n",
      "CASUAL    :\tThe Economy is one of the best in our history , with unemployment at a 50 year low , and the Stock Market ready to again break a record ( set by us many times ) - & all you heard yesterday , based on a phony story , was Impeachment . You want to see a Stock Market Crash , Impeach Trump !\n",
      "SPACY     :\tThe Economy is one of the best in our history , with unemployment at a 50 year low , and the Stock Market ready to again break a record ( set by us many times ) - & amp ; all you heard yesterday , based on a phony story , was Impeachment . You want to see a Stock Market Crash , Impeach Trump !\n",
      "WHITESPACE:\tThe Economy is one of the best in our history, with unemployment at a 50 year low, and the Stock Market ready to again break a record (set by us many times) - &amp; all you heard yesterday, based on a phony story, was Impeachment. You want to see a Stock Market Crash, Impeach Trump!\n",
      "EXTENSIBLE:\tThe Economy is one of the best in our history , with unemployment at a 50 year low , and the Stock Market ready to again break a record ( set by us many times ) - & amp ; all you heard yesterday , based on a phony story , was Impeachment . You want to see a Stock Market Crash , Impeach Trump !\n",
      "\n",
      "NLTK      :\t. @ newtgingrich just stated that there has been no president since Abraham Lincoln who has been treated worse or more unfairly by the media than your favorite President , me ! At the same time there has been no president who has accomplished more in his first two years in office !\n",
      "CASUAL    :\t. @newtgingrich just stated that there has been no president since Abraham Lincoln who has been treated worse or more unfairly by the media than your favorite President , me ! At the same time there has been no president who has accomplished more in his first two years in office !\n",
      "SPACY     :\t.@newtgingrich just stated that there has been no president since Abraham Lincoln who has been treated worse or more unfairly by the media than your favorite President , me ! At the same time there has been no president who has accomplished more in his first two years in office !\n",
      "WHITESPACE:\t.@newtgingrich just stated that there has been no president since Abraham Lincoln who has been treated worse or more unfairly by the media than your favorite President, me! At the same time there has been no president who has accomplished more in his first two years in office!\n",
      "EXTENSIBLE:\t. @newtgingrich just stated that there has been no president since Abraham Lincoln who has been treated worse or more unfairly by the media than your favorite President , me ! At the same time there has been no president who has accomplished more in his first two years in office !\n",
      "\n",
      "NLTK      :\tWill be leaving for Dover to be with the families of 4 very special people who lost their lives in service to our Country !\n",
      "CASUAL    :\tWill be leaving for Dover to be with the families of 4 very special people who lost their lives in service to our Country !\n",
      "SPACY     :\tWill be leaving for Dover to be with the families of 4 very special people who lost their lives in service to our Country !\n",
      "WHITESPACE:\tWill be leaving for Dover to be with the families of 4 very special people who lost their lives in service to our Country!\n",
      "EXTENSIBLE:\tWill be leaving for Dover to be with the families of 4 very special people who lost their lives in service to our Country !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, (one, two, three, four, five) in enumerate(zip(nltk_tokens, nltk_casual_tokens, spacy_tokens, whitespace_tokens, extensible_tokens)):\n",
    "    if idx >= 5:\n",
    "        break\n",
    "    print(\"NLTK      :\\t%s\" % ' '.join(one))\n",
    "    print(\"CASUAL    :\\t%s\" % ' '.join(two))\n",
    "    print(\"SPACY     :\\t%s\" % ' '.join(three))\n",
    "    print(\"WHITESPACE:\\t%s\" % ' '.join(four))\n",
    "    print(\"EXTENSIBLE:\\t%s\" % ' '.join(five))\n",
    "\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: Write a function `compare(tokenization_one, tokenization_two)` that compares two tokenizations of the same text and finds the 20 most frequent tokens that don't appear in the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(one_tokens, two_tokens):\n",
    "    \n",
    "    one_counts=Counter()\n",
    "    two_counts=Counter()\n",
    "\n",
    "    for sentence in one_tokens:\n",
    "        for token in sentence:\n",
    "            one_counts[token]+=1\n",
    "        \n",
    "    for sentence in two_tokens:\n",
    "        for token in sentence:\n",
    "            two_counts[token]+=1\n",
    "        \n",
    "    missing_from_one=Counter()\n",
    "    missing_from_two=Counter()\n",
    "    \n",
    "    for word_type in one_counts:\n",
    "        if word_type not in two_counts:\n",
    "            missing_from_two[word_type]=one_counts[word_type]\n",
    "        \n",
    "    for word_type in two_counts:\n",
    "        if word_type not in one_counts:\n",
    "            missing_from_one[word_type]=two_counts[word_type]\n",
    "\n",
    "    print (\"Token counts -- one: %s, two: %s\" % (len(one_tokens), len(two_tokens)))\n",
    "    print (\"\\nNot in one:\")\n",
    "    print ('\\n'.join(\"%s\\t%d\" % (k,v) for (k,v) in missing_from_one.most_common(20)))\n",
    "    print (\"\\nNot in two:\")\n",
    "    print ('\\n'.join(\"%s\\t%d\" % (k,v) for (k,v) in missing_from_two.most_common(20)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token counts -- one: 36583, two: 36583\n",
      "\n",
      "Not in one:\n",
      "``\t13299\n",
      "''\t11514\n",
      "'s\t3541\n",
      "amp\t3364\n",
      "n't\t2503\n",
      "--\t2077\n",
      "Trump2016\t846\n",
      "U.S.\t665\n",
      "....\t542\n",
      "'m\t538\n",
      "'re\t528\n",
      "CelebApprentice\t416\n",
      "Mr.\t333\n",
      "MittRomney\t312\n",
      "'ve\t307\n",
      "'ll\t307\n",
      "IvankaTrump\t236\n",
      "w/\t209\n",
      "'d\t175\n",
      ".....\t171\n",
      "\n",
      "Not in two:\n",
      "\"\t24807\n",
      "@realDonaldTrump\t8661\n",
      "#Trump2016\t840\n",
      "@BarackObama\t732\n",
      "don't\t626\n",
      "#MakeAmericaGreatAgain\t560\n",
      "@FoxNews\t547\n",
      "I'm\t524\n",
      "@foxandfriends\t504\n",
      "can't\t423\n",
      "@ApprenticeNBC\t393\n",
      "@MittRomney\t314\n",
      "It's\t304\n",
      "it's\t303\n",
      "🇺\t300\n",
      "🇸\t300\n",
      "#CelebApprentice\t289\n",
      "@CNN\t285\n",
      "you're\t276\n",
      "doesn't\t266\n"
     ]
    }
   ],
   "source": [
    "compare(nltk_casual_tokens, nltk_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: Use one of the NLTK tokenizers; write code to determine how many sentences are in this dataset, and what the average number of words per sentence is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sents: 70491, Tokens/sent: 12.6\n"
     ]
    }
   ],
   "source": [
    "count=0.\n",
    "num_sents=0\n",
    "for tweet in tweets:\n",
    "    for sent in nltk.sent_tokenize(tweet):\n",
    "        count+=len(nltk.word_tokenize(sent))\n",
    "        num_sents+=1\n",
    "print(\"Sents: %s, Tokens/sent: %.1f\" % (num_sents, (count/num_sents)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4 (check-plus): modify the extensible tokenizer above to keep urls together (e.g., www.google.com or http://www.google.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep usernames together (any token starting with @, followed by A-Z, a-z, 0-9)\n",
    "regexes=(r\"(?:@[\\w_]+)\",\n",
    "\n",
    "# Keep hashtags together (any token starting with #, followed by A-Z, a-z, 0-9, _, or -)\n",
    "r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\",\n",
    "\n",
    "# Keep urls together\n",
    "r\"(?:https?:\\S+)\",\n",
    "r\"(?:www\\.\\S+)\",\n",
    "  \n",
    "# Keep words with apostrophes, hyphens and underscores together\n",
    "r\"(?:[a-z][a-z’'\\-_]+[a-z])\",\n",
    "\n",
    "# Keep all other sequences of A-Z, a-z, 0-9, _ together\n",
    "r\"(?:[\\w_]+)\",\n",
    "\n",
    "# Everything else that's not whitespace\n",
    "r\"(?:\\S)\"\n",
    ")\n",
    "\n",
    "big_regex=\"|\".join(regexes)\n",
    "\n",
    "my_url_extensible_tokenizer = re.compile(big_regex, re.VERBOSE | re.I | re.UNICODE)\n",
    "\n",
    "def my_extensible_tokenize_with_urls(text):\n",
    "    return my_url_extensible_tokenizer.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('\\n'.join(my_extensible_tokenize_with_urls(\"The course website is http://people.ischool.berkeley.edu/~dbamman/info256.html\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
